{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"En7mw1NQj3Uh"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","\n","!pip install neptune.tensorflow.keras\n","!pip install transformers\n","!pip install sentencepiece\n","\n","from google.colab import drive, auth\n","drive.mount('/content/drive', force_remount=True)\n","auth.authenticate_user()\n","\n","import re\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","\n","import tensorflow as tf\n","tf.config.optimizer.set_jit(True)\n","\n","import neptune.new as neptune\n","\n","from tensorflow.keras.utils import plot_model\n","from transformers import TFRobertaModel, RobertaTokenizer\n","from transformers import TFXLMRobertaModel, XLMRobertaTokenizer\n","from transformers import TFT5EncoderModel\n","from transformers import AdamWeightDecay\n","\n","from glob import glob\n","from pathlib import Path\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/modules')\n","import pipeline, tfmodels, protobuf_handler, valid_analysis, apitokens\n","\n","print('Libraries Imported')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EazrjVlL7GoD"},"outputs":[],"source":["# Initialize TPU Strategy\n","tpu_strategy = pipeline.check_for_tpu_status()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHFUyhjxUkbj"},"outputs":[],"source":["# Model Selection\n","c1_trans_name = 'gbert'\n","c2_trans_name = 'gbert'\n","c3_trans_name = 'multi'\n","\n","model_name = 'gbert_gbert_multi_code_rep'\n","model_objective = 'triple_code_rep'\n","gcs_bucket = 'md_code_rep'\n","\n","model_selection = {'triple_code_rep': [protobuf_handler.decode_triple_protobuf,\n","                                       protobuf_handler.parse_triple_tensor_arrays,\n","                                       tf.keras.losses.MeanSquaredError,\n","                                       'val_loss', ['<d>', '<c>','[DIVIDER]', '[EMPTY]', '[EMOJI]']],\n","                   'dual_code_rep': [protobuf_handler.decode_dual_protobuf,\n","                                     protobuf_handler.parse_dual_tensor_arrays,\n","                                     tf.keras.losses.MeanSquaredError,\n","                                     'val_loss', ['<d>', '<c>', '[DIVIDER]', '[EMPTY]', '[EMOJI]']]}\n","\n","model_selection = model_selection[model_objective]\n","decode_protobuf = model_selection[0]\n","parse_tensor_arrays = model_selection[1]\n","loss_function = model_selection[2]\n","checkpoint_metric = model_selection[3]\n","custom_tokens = model_selection[4]\n","\n","# Additional Checklist:\n","\n","# CHECK TOKEN LOCATION FOR CBERT\n","# 1. Model Class: MDcode, MDOrder, MDExists\n","# 2. Samples per file\n","# 3. Data in GCS-Bucket\n","# 4. Checkpoints and Model Folder\n","\n","params = {'samples_per_file': 55752,\n","          'batch_size': 32,\n","          'learning_rate': 2.5e-5,\n","          'warmup_rate': 0.04,\n","          'epochs': 4}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9wW1T9p_nIFa"},"outputs":[],"source":["# Initalize Model Name and Tokenizer\n","trans_options = {'multi': ['Unbabel/xlm-roberta-comet-small', TFXLMRobertaModel, XLMRobertaTokenizer],\n","                 'cbert': ['microsoft/codebert-base',  TFRobertaModel, RobertaTokenizer],\n","                 'gbert': ['microsoft/graphcodebert-base', TFRobertaModel, RobertaTokenizer],\n","                 't5': ['Salesforce/codet5-base', TFT5EncoderModel, RobertaTokenizer]}  \n","\n","selected_trans = trans_options[c1_trans_name] \n","c1_trans_config = selected_trans[0]\n","c1_model_class = selected_trans[1]\n","c1_tokenizer_class =  selected_trans[2]\n","c1_tokenizer = c1_tokenizer_class.from_pretrained(c1_trans_config)\n","c1_tokenizer = pipeline.add_custom_tokens_to_tokenizer(c1_tokenizer, custom_tokens)\n","\n","selected_trans = trans_options[c2_trans_name] \n","c2_trans_config = selected_trans[0]\n","c2_model_class = selected_trans[1]\n","c2_tokenizer_class =  selected_trans[2]\n","c2_tokenizer = c2_tokenizer_class.from_pretrained(c2_trans_config)\n","c2_tokenizer = pipeline.add_custom_tokens_to_tokenizer(c2_tokenizer, custom_tokens)\n","\n","selected_trans = trans_options[c3_trans_name] \n","c3_trans_config = selected_trans[0]\n","c3_model_class = selected_trans[1]\n","c3_tokenizer_class =  selected_trans[2]\n","c3_tokenizer = c3_tokenizer_class.from_pretrained(c3_trans_config)\n","c3_tokenizer = pipeline.add_custom_tokens_to_tokenizer(c3_tokenizer, custom_tokens + ['\\n'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybkmtbDOnNAL"},"outputs":[],"source":["# Create Train and Valid Datasets\n","auto = tf.data.experimental.AUTOTUNE\n","\n","gcs_records= f'gs://{gcs_bucket}/{model_name}/*.tfrecords'\n","tfrecord_files = tf.io.gfile.glob(gcs_records)\n","train_filepaths = tfrecord_files[:-8]  # 96 total TFRecord files, 16 saved for valid dataset (15%)\n","valid_filepaths = tfrecord_files[-8:]  # Move into train dataset later\n","num_train_samples = params['samples_per_file'] * len(train_filepaths)\n","num_valid_samples = params['samples_per_file'] * len(valid_filepaths)\n","\n","train_dataset = (tf.data.TFRecordDataset(train_filepaths, num_parallel_reads=auto)\n","                  .map(decode_protobuf, num_parallel_calls=auto)\n","                  .map(parse_tensor_arrays, num_parallel_calls=auto)\n","                  .shuffle(num_train_samples+1, seed=1)\n","                  .repeat(params['epochs']+1)\n","                  .batch(params['batch_size'], drop_remainder=True, num_parallel_calls=auto)\n","                  .prefetch(auto))\n","\n","valid_dataset = (tf.data.TFRecordDataset(valid_filepaths, num_parallel_reads=auto)\n","                  .map(decode_protobuf, num_parallel_calls=auto)\n","                  .map(parse_tensor_arrays, num_parallel_calls=auto)\n","                  .shuffle(num_valid_samples+1, seed=1)\n","                  .repeat(params['epochs']+1)\n","                  .batch(params['batch_size'], drop_remainder=True, num_parallel_calls=auto)\n","                  .prefetch(auto))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Zi2NKyNd9Fa"},"outputs":[],"source":["# Set Learning Rate Decayer Parameters\n","train_steps_per_epoch = num_train_samples / params['batch_size']\n","validation_steps_per_epoch = num_valid_samples / params['batch_size']\n","\n","total_steps = train_steps_per_epoch * params['epochs']\n","warmup_steps = int(params['warmup_rate'] * total_steps)                           "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-d6LJq7BCka"},"outputs":[],"source":["# weight_decay_exclude = ['bias', 'LayerNorm.bias', 'LayerNorm.weight', '_norm']\n","# def create_groups_lr_optimizer(lr_str):\n","#   lr_scheduler = tfmodels.WarmupCosineDecay(params['learning_rate']*lr_str, warmup_steps, total_steps)\n","#   lr_optimizer = AdamWeightDecay(lr_scheduler, weight_decay_rate=0.01, exclude_from_weight_decay=weight_decay_exclude)\n","#   return lr_optimizer\n","\n","# grouped_optimizers = [create_groups_lr_optimizer(lr_str) for lr_str in [0.5, 0.75, 1]]\n","\n","# layer_groups = [model.layers[0].layers[0].layers[0].encoder.layer[i:i+4]\n","#                 + model.layers[1].layers[0].layers[0].encoder.layer[i:i+4]\n","#                 for i in range(0, 12, 4)]\n","\n","# optimizers_and_layers = [(grouped_optimizers[0], layer_groups[0]),\n","#                          (grouped_optimizers[1], layer_groups[1]),\n","#                          (grouped_optimizers[2], layer_groups[2] + model.layers[2:])]\n","# lr_optimizer = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n","\n","# if code_trans_name == 't5':\n","#   code_transformer = code_model_class.from_pretrained(code_trans_config,                                               \n","#                                                       output_hidden_states=True,\n","#                                                       from_pt=True,\n","#                                                       dropout_rate=0.2)\n","#   code_transformer.resize_token_embeddings(len(code_tokenizer))\n","#   code_transformer = tfmodels.reinit_weights_and_bias(code_transformer, 5, 't5') "]},{"cell_type":"markdown","metadata":{"id":"OkCtjU2jRIIE"},"source":["### Train Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrbpfhSQnOPt"},"outputs":[],"source":["# Initialize Model\n","with tpu_strategy.scope():\n","  # C1 Transformer\n","  c1_transformer = c1_model_class.from_pretrained(c1_trans_config,\n","                                                  hidden_dropout_prob=0.12,\n","                                                  attention_probs_dropout_prob=0.12)\n","  c1_transformer.resize_token_embeddings(len(c1_tokenizer))\n","  c1_transformer = tfmodels.reinit_weights_and_bias(c1_transformer, 5, 'roberta') \n","\n","  # C2 Transformer\n","  c2_transformer = c2_model_class.from_pretrained(c2_trans_config,\n","                                                  hidden_dropout_prob=0.12,\n","                                                  attention_probs_dropout_prob=0.12)\n","  c2_transformer.resize_token_embeddings(len(c2_tokenizer))\n","  c2_transformer = tfmodels.reinit_weights_and_bias(c2_transformer, 5, 'roberta') \n","\n","  # C3 Transformer\n","  c3_transformer = c3_model_class.from_pretrained(c3_trans_config,\n","                                                  hidden_dropout_prob=0.1,\n","                                                  attention_probs_dropout_prob=0.1,\n","                                                  from_pt=True)\n","  c3_transformer.resize_token_embeddings(len(c3_tokenizer))\n","  c3_transformer = tfmodels.reinit_weights_and_bias(c3_transformer, 3, 'roberta')  # 6 layers in distilled model, just take reinit top 3\n","\n","  model = tfmodels.TripleCodeRep(c1_transformer, c2_transformer, c3_transformer)\n","  \n","  lr_scheduler = tfmodels.WarmupCosineDecayRestarts(params['learning_rate'], warmup_steps, train_steps_per_epoch)\n","  optimizer = AdamWeightDecay(learning_rate=lr_scheduler,\n","                              weight_decay_rate=0.0125,\n","                              exclude_from_weight_decay=['bias', 'LayerNorm', 'norm'])\n","  lr_tracker = tfmodels.LRTracker(optimizer)\n","  metrics = ['mean_squared_error', 'mean_absolute_error', lr_tracker]\n","  \n","  model.compile(optimizer=optimizer, \n","                loss=loss_function(),  # loss_function() or tfmodels.flipped_huber\n","                metrics=metrics,\n","                jit_compile=True)\n","  display(model.build_graph().summary())\n","  display(plot_model(model.build_graph()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMDAOJgmJ8-1"},"outputs":[],"source":["# Initalize Neptune Metadata Tracker Callback\n","neptune_token = apitokens.neptune_token\n","\n","run = neptune.init(project=\"robby700/GCode\",\n","                    name=\"Google AI4Code Challenge\",\n","                    tags=[f'c1_{c1_trans_name}',\n","                          f'c2_{c2_trans_name}',\n","                          f'c3_{c3_trans_name}', \n","                          f'{model_objective}'],\n","                    api_token=neptune_token,\n","                    capture_hardware_metrics=False)\n","\n","run['hyper_parameters'] = params\n","neptune_cbk = tfmodels.NeptuneCallback(run=run)\n","\n","# Initalize Model Checkpoint Callback\n","checkpoint_filepath = f'gs://{gcs_bucket}/checkpoints/{model_name}'\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n","                                                               monitor=checkpoint_metric,\n","                                                               verbose=1,\n","                                                               save_weights_only=False,\n","                                                               save_best_only=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q82Tt1UznPqp"},"outputs":[],"source":["# Train Model\n","history = model.fit(train_dataset,\n","                    validation_data=valid_dataset,\n","                    steps_per_epoch=train_steps_per_epoch, \n","                    validation_steps=validation_steps_per_epoch,\n","                    epochs=params['epochs'],\n","                    callbacks=[neptune_cbk, model_checkpoint_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PT6ZC0Xgyplv"},"outputs":[],"source":["# Stop Neptune Run\n","run.stop()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dLyh3FaXzACn"},"outputs":[],"source":["# Save Model\n","model_save_path =  f'gs://{gcs_bucket}/models/{model_name}'\n","model_with_opt_save_path =  f'gs://{gcs_bucket}/models_with_opt/{model_name}'\n","\n","tf.keras.models.save_model(model, model_save_path, include_optimizer=False)\n","tf.keras.models.save_model(model, model_with_opt_save_path, include_optimizer=True)\n","print(f\"Model and weights have been saved at {model_save_path}\")"]},{"cell_type":"markdown","metadata":{"id":"Cgzdi9EUP9_R"},"source":["### Reload Model for Resumed Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nrYXmGmrtaFJ"},"outputs":[],"source":["# # Load in saved models\n","\n","# # Hard reset cosine for last batch\n","# # If you keep batch size 32, might be able to keep adam optimizer weights\n","# # Decay only at end, let's reset it\n","\n","# model_path = f'gs://{gcs_bucket}/checkpoints/{model_name}'\n","# neptune_run_name = 'GCOD-226' \n","# current_epoch = 2\n","\n","# with tpu_strategy.scope():\n","#   lr_scheduler = {'AdamWeightDecay': AdamWeightDecay,\n","#                   'WarmupCosineDecayRestarts': tfmodels.WarmupCosineDecayRestarts,\n","#                   'LRTracker': tfmodels.LRTracker}\n","#   load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n","\n","#   model = tf.keras.models.load_model(model_path,\n","#                                      custom_objects=lr_scheduler,\n","#                                      options=load_options,\n","#                                      compile=True)\n","#   print('Loaded in model')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tC81M5ym-x5g"},"outputs":[],"source":["# Can we do a quick change and just straight up add the warmup back in on each runup?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F5TULyyC6Tuz"},"outputs":[],"source":["# Reload Model with Weights\n","\n","neptune_run_name = 'GCOD-240'\n","current_epoch = 2\n","model_save_path = f'gs://{gcs_bucket}/models_with_opt/{model_name}'\n","\n","with tpu_strategy.scope():\n","  # C1 Transformer\n","  c1_transformer = c1_model_class.from_pretrained(c1_trans_config,\n","                                                  output_hidden_states=True,\n","                                                  hidden_dropout_prob=0.12,\n","                                                  attention_probs_dropout_prob=0.12)\n","  c1_transformer.resize_token_embeddings(len(c1_tokenizer))\n","\n","  # C2 Transformer\n","  c2_transformer = c2_model_class.from_pretrained(c2_trans_config,\n","                                                  output_hidden_states=True,\n","                                                  hidden_dropout_prob=0.12,\n","                                                  attention_probs_dropout_prob=0.12)\n","  c2_transformer.resize_token_embeddings(len(c2_tokenizer))\n","\n","  # C3 Transformer\n","  c3_transformer = c3_model_class.from_pretrained(c3_trans_config,\n","                                                  output_hidden_states=True,\n","                                                  hidden_dropout_prob=0.1,\n","                                                  attention_probs_dropout_prob=0.1,\n","                                                  from_pt=True)\n","  c3_transformer.resize_token_embeddings(len(c3_tokenizer))\n","  model = tfmodels.TripleCodeRep(c1_transformer, c2_transformer, c3_transformer)\n","\n","  lr_scheduler = tfmodels.WarmupCosineDecayRestarts(params['learning_rate'], warmup_steps, train_steps_per_epoch)                                          \n","  optimizer = AdamWeightDecay(learning_rate=lr_scheduler,\n","                              weight_decay_rate=0.0125,\n","                              exclude_from_weight_decay=['bias', 'LayerNorm', 'norm'])\n","  lr_tracker = tfmodels.LRTracker(optimizer)\n","  metrics = ['mean_squared_error', 'mean_absolute_error', lr_tracker]\n","  \n","  model.compile(optimizer=optimizer, \n","                loss=loss_function(),  # loss_function() or tfmodels.flipped_huber\n","                metrics=metrics,\n","                jit_compile=True)\n","  model.load_weights(model_save_path)\n","  print('Loading in model and weights')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZP973Xv_RZ45"},"outputs":[],"source":["# Initalize Neptune Metadata Tracker Callback\n","neptune_token = apitokens.neptune_token\n","\n","run = neptune.init(project=\"robby700/GCode\",\n","                   api_token=neptune_token,\n","                   capture_hardware_metrics=False,\n","                   run=neptune_run_name)\n","neptune_cbk = tfmodels.NeptuneCallback(run=run)\n","\n","# Initalize Model Checkpoint Callback\n","checkpoint_filepath = f'gs://{gcs_bucket}/checkpoints/{model_name}'\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n","                                                               monitor=checkpoint_metric,\n","                                                               verbose=1,\n","                                                               save_weights_only=False,\n","                                                               save_best_only=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HGkM6tlRQzP5"},"outputs":[],"source":["# Start training up again\n","history = model.fit(train_dataset,\n","                    validation_data=valid_dataset,\n","                    steps_per_epoch=train_steps_per_epoch, \n","                    validation_steps=validation_steps_per_epoch,\n","                    epochs=params['epochs'],\n","                    initial_epoch=current_epoch,\n","                    callbacks=[neptune_cbk, model_checkpoint_callback])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vqUYWwF9RQip"},"outputs":[],"source":["run.stop()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59u5WPNLD56h"},"outputs":[],"source":["# Save Model\n","model_save_path =  f'gs://{gcs_bucket}/models/{model_name}'\n","model_with_opt_save_path =  f'gs://{gcs_bucket}/models_with_opt/{model_name}'\n","\n","tf.keras.models.save_model(model, model_save_path, include_optimizer=False)\n","tf.keras.models.save_model(model, model_with_opt_save_path, include_optimizer=True)\n","print(f\"Model and weights have been saved at {model_save_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-2Fp485cA1yq"},"outputs":[],"source":["# Load in weights\n","# weight_save_path = f'/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/saved_models/opt_weights/{model_name}_opt_weights.npy'\n","# opt_weights = np.load(weight_save_path, allow_pickle=True)\n","# grad_vars = model.get_weights()\n","# zero_grads = [tf.zeros_like(w) for w in grad_vars]\n","\n","# Set zero gradients which don't do nothing for Adam\n","# optimizer.apply_gradients(zip(zero_grads, grad_vars))\n","\n","# Set the weights of the optimizer\n","# model.optimizer.set_weights(opt_weights)"]},{"cell_type":"markdown","metadata":{"id":"gJolg_jgqxLL"},"source":["### Analyze Predictions for Threshold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FUfurtMhjFq4"},"outputs":[],"source":["# Double check all features have been parased and prepared correctly\n","\n","for batch_features, batch_labels in train_dataset.take(1):\n","  print(f'Train Dataset Tensor Spec:\\n{train_dataset}\\n')\n","  print(f'Parsed Input Ids:\\n{batch_features[0]}\\n')\n","  print(f'Parsed Attention Mask:\\n{batch_features[1]}\\n')\n","  print(f'Parsed Labels:\\n{batch_labels}\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gAR_NbVf1xvV"},"outputs":[],"source":["# Check predictions on a single batch\n","predictions = model.predict(batch_features, verbose=2)\n","print(f'\\nPredictions with dtype {predictions.dtype}:\\n{predictions}')\n","print(f'\\nTrue Labels wtih dtype {batch_labels.dtype}:\\n{batch_labels}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVxt73DhqE2F"},"outputs":[],"source":["# Collect valid labels and run predictions for comparison, this takes a while around 20-25 minutes\n","valid_labels = valid_analysis.get_labels(valid_dataset)\n","valid_predictions = model.predict(valid_dataset, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGkhqsb3n6ad"},"outputs":[],"source":["# Print ROC Curve and Threshold\n","roc_auc, best_threshold = valid_analysis.plot_roc_curve(valid_labels, valid_predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kOHrEYs3xHpA"},"outputs":[],"source":["# Calculate and plot confusion matrix at best threshold from ROC\n","binary_valid_preds = np.where(valid_predictions >= best_threshold, 1, 0)\n","\n","tn, fp, fn, tp = valid_analysis.confusion_matrix(valid_labels, binary_valid_preds).ravel()\n","precision = tp/(tp+fp)\n","recall = tp/(tp+fn)\n","\n","cm, cm_perc = valid_analysis.plot_confusion_matrix(valid_labels, binary_valid_preds)\n","print(f'At an optimal ROC threshold of {best_threshold:.2f}. Precision is {precision:.2f} and recall is {recall:.2f}.\\n')"]}],"metadata":{"accelerator":"TPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNKeRfmq3ZCiWK5CRYZliWV"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}