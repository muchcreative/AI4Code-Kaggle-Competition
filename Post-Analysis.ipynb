{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7qLcXJmyLSij"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","\n","!pip install transformers\n","!pip install sentencepiece\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","import random\n","import math\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import tensorflow as tf\n","tf.config.optimizer.set_jit(True)\n","\n","from tqdm.auto import tqdm\n","from transformers import AdamWeightDecay\n","from transformers import XLMRobertaTokenizer, RobertaTokenizer, T5Tokenizer\n","\n","import os\n","from pathlib import Path\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/modules')\n","import pipeline, post_processing, tfmodels, dual_code_rep_fc, triple_code_rep_fc\n","\n","print('Libraries Imported')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FuI19sOFIzgB"},"outputs":[],"source":["# Check GPU Model and Status\n","# A100 > V100 > T4 > P100\n","pipeline.check_for_gpu_status()\n","gpu_strategy = tf.distribute.MirroredStrategy()\n","\n","!nvidia-smi\n","!nvidia-smi --query-gpu=gpu_name, driver_version, memory.total --format=csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VxUjQvjo5r7u"},"outputs":[],"source":["# Setup models to analyze\n","\n","c1_trans_model = 'gbert'\n","c2_trans_model = 'gbert'\n","c3_trans_model = 'multi'\n","model_name = 'gbert_gbert_multi_code_rep'\n","\n","params = {'batch_size': 64}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"728PYzLStVGL"},"outputs":[],"source":["# Initalize Model Name and Tokenizer with Custom Tokens\n","trans_options = {'multi': ['Unbabel/xlm-roberta-comet-small', XLMRobertaTokenizer],\n","                 'cbert': ['microsoft/codebert-base',  RobertaTokenizer],\n","                 'gbert': ['microsoft/graphcodebert-base', RobertaTokenizer],\n","                 't5': ['Salesforce/codet5-base', RobertaTokenizer]}            \n","custom_tokens = ['<d>', '<c>', '[DIVIDER]', '[EMPTY]', '[EMOJI]']\n","\n","# C1 Model\n","c1_options = trans_options[c1_trans_model] \n","c1_trans_name = c1_options[0]\n","c1_tokenizer_class = c1_options[1]\n","c1_tokenizer = c1_tokenizer_class.from_pretrained(c1_trans_name)\n","c1_tokenizer = pipeline.add_custom_tokens_to_tokenizer(c1_tokenizer, custom_tokens)\n","\n","# C2 Model\n","c2_options = trans_options[c2_trans_model] \n","c2_trans_name = c2_options[0]\n","c2_tokenizer_class = c2_options[1]\n","c2_tokenizer = c2_tokenizer_class.from_pretrained(c2_trans_name)\n","c2_tokenizer = pipeline.add_custom_tokens_to_tokenizer(c2_tokenizer, custom_tokens)\n","\n","# C3 Modle\n","c3_options = trans_options[c3_trans_model] \n","c3_trans_name = c3_options[0]\n","c3_tokenizer_class = c3_options[1]\n","c3_tokenizer = c3_tokenizer_class.from_pretrained(c3_trans_name)\n","c3_tokenizer = pipeline.add_custom_tokens_to_tokenizer(c3_tokenizer, custom_tokens + ['\\n'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zX1yDYEShqdU"},"outputs":[],"source":["# Unzip files from drive to disk\n","data_path = '/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/data/AI4Code.zip'\n","disk_path = '/content/AI4Code'\n","\n","pipeline.unzip_files(data_path, disk_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jMBRLi1q7zq_"},"outputs":[],"source":["# Load in saved models\n","model_path = f'/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/saved_models/{model_name}'\n","\n","with gpu_strategy.scope():\n","  lr_scheduler = {'AdamWeightDecay': AdamWeightDecay,\n","                  'WarmupCosineDecayRestarts': tfmodels.WarmupCosineDecay,\n","                  'LRTracker': tfmodels.LRTracker}\n","  load_options = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n","\n","  model = tf.keras.models.load_model(model_path,\n","                                     custom_objects=lr_scheduler,\n","                                     options=load_options,\n","                                     compile=False)\n","  print('Loaded in model')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jazTATsEjI-l"},"outputs":[],"source":["test_ids_path = '/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/data/test_ids.yaml'\n","test_ids = pipeline.load_yaml_file(test_ids_path)['ids']\n","\n","data_dir = Path(disk_path)\n","test_paths = [(data_dir/'train'/(test_id+'.json')) for test_id in test_ids]\n","\n","print(f\"There are {len(test_paths)} files after removing outliers and similar files.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YY0AUc2RiuYQ"},"outputs":[],"source":["# We should just load it all in to see what it all looks like\n","def read_notebook(path):\n","    return (pd.read_json(path, dtype={'cell_type': 'category', 'source': 'str'})\n","              .assign(id=path.stem)\n","              .rename_axis('cell_id'))\n","    \n","notebooks = [read_notebook(path) for path in tqdm(test_paths, desc='Train NBs')]\n","df = (pd.concat(notebooks)\n","        .set_index('id', append=True)\n","        .swaplevel()\n","        .sort_index(level='id', sort_remaining=False))\n","\n","df_orders = pd.read_csv(data_dir / 'train_orders.csv',\n","                        index_col='id',\n","                        squeeze=True).str.split()                      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H9nFaH1rW4kq"},"outputs":[],"source":["# How many doc ids do you want to test?\n","doc_ids = test_ids[10000:18000]"]},{"cell_type":"markdown","metadata":{"id":"WNkGiw42tPK3"},"source":["## Assemble Predicted Order"]},{"cell_type":"markdown","metadata":{"id":"XtLpcezbSRjK"},"source":["### Markdown Code Rep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v6_ELNXPSTq8"},"outputs":[],"source":["pred_map = {}\n","dupes = {}\n","dataset_len = 0\n","\n","c1_ii = []\n","c1_am = []\n","\n","c2_ii = []\n","c2_am = []\n","\n","c3_ii = []\n","c3_am = []\n","\n","for doc_id in tqdm(doc_ids, desc='Preparing Mardown Code Rep Features'):\n","  current_doc = df.loc[doc_id]\n","  code_markdown_locs, text = current_doc['cell_type'], current_doc['source']\n","  cell_metadata = [code_markdown_locs.to_dict()]\n","  md_count = code_markdown_locs.value_counts()['markdown']\n","  code_count = code_markdown_locs.value_counts()['code']\n","  \n","  text = pipeline.preprocess_text(text, code_markdown_locs, disable_print=True)\n","  c1_input_ids = pipeline.encode_text_for_input_ids(text, c1_tokenizer, disable_print=True)\n","  c2_input_ids = pipeline.encode_text_for_input_ids(text, c2_tokenizer, disable_print=True)\n","  c3_input_ids = pipeline.encode_text_for_input_ids(text, c3_tokenizer, disable_print=True)\n","  c1_groupings, c2_groupings, c3_groupings, md_dupes = triple_code_rep_fc.collect_best_code_groupings(c1_input_ids,\n","                                                                                                      c2_input_ids,\n","                                                                                                      c3_input_ids,\n","                                                                                                      cell_metadata)\n","  c1_features = triple_code_rep_fc.create_features(c1_groupings, c1_tokenizer, disable_print=True)\n","  c2_features = triple_code_rep_fc.create_features(c2_groupings, c2_tokenizer, disable_print=True)\n","  c3_features = triple_code_rep_fc.create_features(c3_groupings, c3_tokenizer, disable_print=True)\n"," \n","  if md_dupes:\n","    dupes[doc_id] = md_dupes\n","  \n","  if len(c1_groupings) == 0:\n","    pred_map[doc_id] = 0\n","  else:\n","    pred_map[doc_id] = 1\n","    \n","    c1_ii.append(c1_features[0])\n","    c1_am.append(c1_features[1])\n","\n","    c2_ii.append(c2_features[0])\n","    c2_am.append(c2_features[1])\n","\n","    c3_ii.append(c3_features[0])\n","    c3_am.append(c3_features[1])\n","    dataset_len += len(c1_groupings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XX--Tq0Cr8VS"},"outputs":[],"source":["# MD Code Rep Predictions\n","base_features = tf.data.Dataset.from_tensor_slices((np.concatenate(c1_ii, axis=0, dtype=np.int32),\n","                                                    np.concatenate(c1_am, axis=0, dtype=np.int32),\n","                                                    np.concatenate(c2_ii, axis=0, dtype=np.int32),\n","                                                    np.concatenate(c2_am, axis=0, dtype=np.int32),\n","                                                    np.concatenate(c3_ii, axis=0, dtype=np.int32),\n","                                                    np.concatenate(c3_am, axis=0, dtype=np.int32)))\n","dummy_labels = tf.data.Dataset.from_tensor_slices(np.zeros((dataset_len, 1), dtype=np.float32))\n","dataset = (tf.data.Dataset.zip((base_features, dummy_labels))\n","            .batch(params['batch_size']))\n","\n","pred_steps = math.ceil(dataset_len / params['batch_size'])\n","preds = model.predict(dataset, steps=pred_steps, verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0T940qesF-f"},"outputs":[],"source":["# Assemble Doc Order\n","pred_start = 0\n","y_preds = {}\n","\n","for doc_id in tqdm(doc_ids, desc='Assembling Doc Order'):\n","  code_markdown_locs = df.loc[doc_id]['cell_type']\n","  pred_count_per_md = pred_map[doc_id]\n","    \n","  try:\n","    md_dupes = dupes[doc_id]\n","  except KeyError:\n","    md_dupes = []\n","\n","  code_count = code_markdown_locs.value_counts()['code']\n","  code_pct_ranks = list(np.arange(1, code_count+1, dtype=np.float32))\n","\n","  md_count = code_markdown_locs.value_counts()['markdown']\n","  md_pct_ranks = []\n","\n","  pred_end = pred_start + (md_count-len(md_dupes))\n","  doc_mc_preds = preds[pred_start:pred_end]\n","  mini_pred_start = 0\n","  default_dupe_loc = 0.54288805 * (code_count-1)  # Avg dupe location is at 0.54288805\n","\n","  for i in range(md_count):\n","    if i in md_dupes:\n","      md_pct_ranks.append(default_dupe_loc)\n","    else:\n","      rel_pred_rank = doc_mc_preds[mini_pred_start:mini_pred_start+1]\n","      rank = (rel_pred_rank + 1)/2 * (code_count+1)\n","      mini_pred_start += 1\n","      md_pct_ranks.append(rank)\n","\n","  metadata_df = code_markdown_locs.to_frame()\n","  pct_ranks =  code_pct_ranks + md_pct_ranks\n","  metadata_df['pct_rank'] = pct_ranks\n","\n","  pred_order = metadata_df.sort_values(\"pct_rank\").index.tolist()\n","  y_preds[doc_id] = pred_order\n","  pred_start += (md_count - len(md_dupes))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rma0PIR52Hjd"},"outputs":[],"source":["# Calculate Kendall Tau Scores\n","y_preds = pd.Series(y_preds)\n","ground_truth = df_orders.loc[doc_ids]\n","kendall_tau_score = post_processing.calculate_kendall_tau(ground_truth, y_preds)\n","print(f\"The Kendall Tau Scores for the documents was {kendall_tau_score:.4f}.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"24WE3h84b_9l"},"outputs":[],"source":["# Last one was at 86.39 kendall tau same as before"]},{"cell_type":"markdown","metadata":{"id":"eTuAd6EFfH8w"},"source":["### Kendall Tau Post Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6avua676fFca"},"outputs":[],"source":["# Collect highest Kendall-Tau's and Lowest Scores, let's get their doc ids\n","kendall_taus = []\n","\n","for gt, y_pred in zip(ground_truth, y_preds):\n","  kendall_taus.append(post_processing.calculate_kendall_tau([gt], [y_pred]))\n","\n","kendall_taus = np.asarray(kendall_taus)\n","print(f'The mean score was {kendall_taus.mean():.4f} and the median score was {np.median(kendall_taus):.4f}.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mKiWqG-8fGUm"},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(8, 6))\n","ax = sns.histplot(kendall_taus, ax=ax)\n","_ = ax.set(title='Kendall Tau Score Distribution')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQwUzE5ifHS-"},"outputs":[],"source":["# Collect Worst and Highest Ids\n","# Looking into this, the outputs share the same doc ids, unsure\n","\n","highest_idx = []\n","lowest_idx = []\n","\n","highest_kt = np.sort(kendall_taus)[::-1][:1000]\n","lowest_kt = np.sort(kendall_taus)[:1000]\n","print(f\"Highest Scores are {highest_kt[:5]}\")\n","print(f\"\\nLowest Scores are {lowest_kt[:5]}\")\n","\n","for i in range(len(highest_kt)):\n","  highest_idx.append(np.where(kendall_taus == highest_kt[i])[0])\n","  lowest_idx.append(np.where(kendall_taus == lowest_kt[i])[0])\n","\n","highest_idx = np.concatenate(highest_idx)\n","lowest_idx = np.concatenate(lowest_idx)\n","\n","doc_ids = np.asarray(doc_ids)\n","highest_doc_id = doc_ids[highest_idx]\n","lowest_doc_id = doc_ids[lowest_idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1lPTagkdfK4r"},"outputs":[],"source":["selected_id = lowest_doc_id[10]\n","ordered_df = post_processing.get_ordered_df(selected_id, df, df_orders)\n","display(ordered_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yVBSWMIdfLvy"},"outputs":[],"source":["predicted_order = y_preds.loc[selected_id]\n","predicted_df = df.loc[selected_id].loc[predicted_order]\n","display(df.loc[selected_id].loc[predicted_order])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NBah1wnGdBus"},"outputs":[],"source":["pipeline.preprocess_text(df.loc[selected_id].loc[predicted_order]['source'],\n","                         df.loc[selected_id].loc[predicted_order]['cell_type'], \n","                         disable_print=False)"]},{"cell_type":"markdown","metadata":{"id":"_OlIjTcdLXz6"},"source":["### Check Layer Weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M08CHYM9Ld3L"},"outputs":[],"source":["def plot_layer_weights(layer_weights, cmap):\n","  fig, ax = plt.subplots(figsize=(20,8))\n","  layer_max = np.amax(layer_weights)\n","  layer_min = np.amin(layer_weights)\n","  ax = sns.heatmap(data=layer_weights, cmap=cmap)\n","  ax.set(title=f'Min and Max at [{layer_min:.7f}, {layer_max:.7f}]')\n","  return\n","\n","cmaps = ['Blues', 'viridis', 'Spectral', 'rocket', 'mako']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u3Uwnb2esl2L"},"outputs":[],"source":["md_mean_weights = []\n","\n","# CLS\n","for i in range(1536):\n","  md_mean_weights.append(model.layers[2].get_weights()[2][i].mean())\n","\n","md_mean_weights = np.asarray(md_mean_weights, dtype=np.float32).reshape(1, 1536)\n","plot_layer_weights(md_mean_weights, cmap='Blues')\n","print(f\"The markdown transformer mean weights are {md_mean_weights.mean():.7f} and median at {np.median(md_mean_weights):.7f}.\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2i5qv2iIx0Ov"},"outputs":[],"source":["code_mean_weights = []\n","\n","# DOC\n","for i in range(1536):\n","  code_mean_weights.append(model.layers[3].get_weights()[2][i].mean())\n","\n","code_mean_weights = np.asarray(code_mean_weights, dtype=np.float32).reshape(1, 1536)\n","plot_layer_weights(code_mean_weights, cmap='Blues')\n","print(f\"The quadrant transformer mean weights are {code_mean_weights.mean():.7f} and median at {np.median(code_mean_weights):.7f}.\\n\")"]},{"cell_type":"markdown","metadata":{"id":"xLB6msZUY1VT"},"source":["### Kaggle Check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JMuz8lBaXRsL"},"outputs":[],"source":["# def read_notebook(path):\n","#     return (pd.read_json(path, dtype={'cell_type': 'category', 'source': 'str'})\n","#               .assign(id=path.stem)\n","#               .rename_axis('cell_id'))\n","\n","# def create_df(notebooks):\n","#     df = (pd.concat(notebooks)\n","#             .set_index('id', append=True)\n","#             .swaplevel()\n","#             .sort_index(level='id', sort_remaining=False))\n","#     return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywNLEwlnY3Dm"},"outputs":[],"source":["# Create Features and Split Test Dataset Into Two\n","\n","y_preds = {}\n","file_amount = len(doc_ids)\n","path_splits = [0, int(file_amount*0.25), int(file_amount*0.5), int(file_amount*0.75), file_amount]\n","\n","for split in range(4):\n","    start_idx = path_splits[split]\n","    end_idx = path_splits[split+1]\n","    current_paths = doc_ids[start_idx:end_idx]\n","\n","    # notebooks = [read_notebook(path) for path in tqdm(current_paths, desc='Test NBs')]\n","    # df = create_df(notebooks)\n","    # doc_ids = df.index.unique(level=0)\n","    \n","    pred_map = {}\n","    dupes = {}\n","    pred_start = 0\n","    dataset_len = 0\n","\n","    c1_ii = []\n","    c1_am = []\n","\n","    c2_ii = []\n","    c2_am = []\n","\n","    c3_ii = []\n","    c3_am = []\n","    \n","    for doc_id in tqdm(current_paths, desc='Preparing Mardown Code Rep Features'):\n","        current_doc = df.loc[doc_id]\n","        code_markdown_locs, text = current_doc['cell_type'], current_doc['source']\n","        cell_metadata = [code_markdown_locs.to_dict()]\n","        md_count = code_markdown_locs.value_counts()['markdown']\n","        code_count = code_markdown_locs.value_counts()['code']\n","  \n","        text = pipeline.preprocess_text(text, code_markdown_locs, disable_print=True)\n","        c1_input_ids = pipeline.encode_text_for_input_ids(text, c1_tokenizer, disable_print=True)\n","        c2_input_ids = pipeline.encode_text_for_input_ids(text, c2_tokenizer, disable_print=True)\n","        c3_input_ids = pipeline.encode_text_for_input_ids(text, c3_tokenizer, disable_print=True)\n","        c1_groupings, c2_groupings, c3_groupings, md_dupes = triple_code_rep_fc.collect_best_code_groupings(c1_input_ids,\n","                                                                                                            c2_input_ids,\n","                                                                                                            c3_input_ids,\n","                                                                                                            cell_metadata)\n","        c1_features = triple_code_rep_fc.create_features(c1_groupings, c1_tokenizer, disable_print=True)\n","        c2_features = triple_code_rep_fc.create_features(c2_groupings, c2_tokenizer, disable_print=True)\n","        c3_features = triple_code_rep_fc.create_features(c3_groupings, c3_tokenizer, disable_print=True)\n"," \n","        if md_dupes:\n","            dupes[doc_id] = md_dupes\n","        \n","        if len(c1_groupings) == 0:\n","            pred_map[doc_id] = 0\n","        else:\n","            pred_map[doc_id] = 1\n","    \n","            c1_ii.append(c1_features[0])\n","            c1_am.append(c1_features[1])\n","    \n","            c2_ii.append(c2_features[0])\n","            c2_am.append(c2_features[1])\n","\n","            c3_ii.append(c3_features[0])\n","            c3_am.append(c3_features[1])\n","            dataset_len += len(c1_groupings)\n","            \n","    # MC Rep Predictions\n","    base_features = tf.data.Dataset.from_tensor_slices((np.concatenate(c1_ii, axis=0, dtype=np.int32),\n","                                                        np.concatenate(c1_am, axis=0, dtype=np.int32),\n","                                                        np.concatenate(c2_ii, axis=0, dtype=np.int32),\n","                                                        np.concatenate(c2_am, axis=0, dtype=np.int32),\n","                                                        np.concatenate(c3_ii, axis=0, dtype=np.int32),\n","                                                        np.concatenate(c3_am, axis=0, dtype=np.int32)))\n","    dummy_labels = tf.data.Dataset.from_tensor_slices(np.zeros((dataset_len, 1), dtype=np.float32))\n","    dataset = (tf.data.Dataset.zip((base_features, dummy_labels))\n","                .batch(params['batch_size']))\n","\n","    pred_steps = math.ceil(dataset_len / params['batch_size'])\n","    preds = model.predict(dataset, steps=pred_steps, verbose=1)\n","    \n","    # Assemble Doc Order\n","    for doc_id in tqdm(current_paths, desc='Assembling Doc Order'):\n","        code_markdown_locs = df.loc[doc_id]['cell_type']\n","        pred_count_per_md = pred_map[doc_id]\n","    \n","        try:\n","            md_dupes = dupes[doc_id]\n","        except KeyError:\n","            md_dupes = []\n","\n","        code_count = code_markdown_locs.value_counts()['code']\n","        md_count = code_markdown_locs.value_counts()['markdown']\n","\n","        pred_end = pred_start + ((md_count-len(md_dupes))*pred_count_per_md)\n","        doc_mc_preds = preds[pred_start:pred_end]\n","\n","        code_pct_ranks = list(np.arange(1, code_count+1, dtype=np.float32))\n","\n","        md_pct_ranks = []\n","        mini_pred_start = 0\n","        default_dupe_loc = 0.54263765 * (code_count - 1) # Avg dupe location is at 0.54263765 \n","\n","        for i in range(md_count):\n","            if i in md_dupes:\n","                md_pct_ranks.append(default_dupe_loc)\n","            else:\n","                rel_pred_rank = doc_mc_preds[mini_pred_start:mini_pred_start+1]\n","                rank = (rel_pred_rank + 1)/2 * (code_count+1)\n","                mini_pred_start += 1\n","                md_pct_ranks.append(rank)\n","\n","        metadata_df = code_markdown_locs.to_frame()\n","        pct_ranks =  code_pct_ranks + md_pct_ranks\n","        metadata_df['pct_rank'] = pct_ranks\n","\n","        pred_order = metadata_df.sort_values(\"pct_rank\").index.tolist()\n","        #y_preds[doc_id] = ' '.join(pred_order)\n","        y_preds[doc_id] = pred_order\n","        pred_start += (md_count - len(md_dupes))*pred_count_per_md"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RK4_Ag3M0usr"},"outputs":[],"source":["# Calculate Kendall Tau Scores\n","y_preds = pd.Series(y_preds)\n","ground_truth = df_orders.loc[doc_ids]\n","kendall_tau_score = post_processing.calculate_kendall_tau(ground_truth, y_preds)\n","print(f\"The Kendall Tau Scores for the documents was {kendall_tau_score:.4f}.\")"]},{"cell_type":"code","source":["#  Here 0.8639"],"metadata":{"id":"i_gYaxXAyz9K"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":["_OlIjTcdLXz6"],"machine_shape":"hm","name":"Post-Analysis.ipynb","provenance":[],"authorship_tag":"ABX9TyPJn2Qb+ltHg+YVwd4NexcS"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}