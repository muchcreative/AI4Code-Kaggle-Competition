{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"k9qjxpbZuSqp"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2\n","\n","!pip install transformers\n","!pip install sentencepiece\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","import re\n","import random\n","import numpy as np\n","import pandas as pd\n","\n","import tensorflow as tf\n","tf.config.optimizer.set_jit(True)\n","\n","from tqdm.auto import tqdm\n","from transformers import XLMRobertaTokenizer, BartTokenizer, RobertaTokenizer, T5Tokenizer\n","\n","import os\n","import yaml\n","from pathlib import Path\n","\n","import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/modules')\n","import pipeline, protobuf_handler, dual_code_rep_fc, data_cleaner, triple_code_rep_fc\n","\n","print('Libraries Imported')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1659937927347,"user":{"displayName":"Robby Lem","userId":"00925855096223384832"},"user_tz":360},"id":"8pwccrJquZ2G","outputId":"79d07ad1-816c-445c-b449-54150cb0ee82"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files have already been unzipped to disk path.\n","Files have already been unzipped to disk path.\n","Files have already been unzipped to disk path.\n"]}],"source":["# Unzip files from drive to disk\n","main_path = '/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/data/AI4Code.zip'\n","kag_path = '/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/data/kag_train.zip'\n","jup_path = '/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/data/jup_train.zip'\n","\n","main_disk_path = '/content/AI4Code'\n","kag_disk_path = '/content/kag_train'\n","jup_disk_path = '/content/jup_train'\n","\n","pipeline.unzip_files(main_path, main_disk_path)\n","pipeline.unzip_files(kag_path, kag_disk_path)\n","pipeline.unzip_files(jup_path, jup_disk_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LnIpfZE-wUvn"},"outputs":[],"source":["# Get correct order of documents\n","main_data_dir = Path('/content/AI4Code')\n","df_orders = pd.read_csv(main_data_dir / 'train_orders.csv',\n","                        index_col='id',\n","                        squeeze=True).str.split()\n","\n","kag_orders_path = '/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/data/kag_orders.json'\n","kag_orders = pd.read_json(kag_orders_path, typ='series')\n","\n","jup_orders_path = '/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/data/jup_orders.json'\n","jup_orders = pd.read_json(jup_orders_path, typ='series')\n","\n","df_orders = pd.concat([df_orders, kag_orders, jup_orders])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pKpsfodAqiDD"},"outputs":[],"source":["# Load in excluded IDs and drop them from the data filepath\n","# Avaliable options: 'similars', 'dupe_md', 'single_code_cell', 'less_than_five', 'less_than_eight', 'less_than_four', 'single_md', 'non_english_ids', 'outliers'\n","\n","excluded_ids_path = '/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/data/excluded_ids.yaml'\n","outlier_ids_path = '/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/data/outlier_ids.yaml'\n","\n","similar_ids = pipeline.load_yaml_file(excluded_ids_path)['similars']\n","outlier_ids = pipeline.load_yaml_file(outlier_ids_path)['ids']\n","\n","random_seed = 20\n","data_paths = list((main_data_dir / 'train').glob('*.json'))\n","data_paths = pipeline.remove_id_paths(data_paths, similar_ids)\n","\n","kag_paths = list(Path(kag_disk_path).glob('*.csv'))\n","jup_paths = list(Path(jup_disk_path).glob('*.csv'))\n","data_paths = data_paths + kag_paths + jup_paths\n","\n","data_paths = pipeline.remove_id_paths(data_paths, outlier_ids)\n","random.Random(random_seed).shuffle(data_paths)  # Shuffle data paths list\n","\n","all_dataset_splits = np.linspace(0, len(data_paths), num=97, dtype=np.int32)\n","print(f\"Training with {all_dataset_splits}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_XOtyLTIYhnK"},"outputs":[],"source":["# Select Feature Creation Type and Transformer Model\n","# Mean tokens in a markdown cell is 112.53 and median is 50.0\n","# 11 markdown count per document median and 15 mean\n","\n","# Mean tokens in a code cell is 43.66 and median is 15.0\n","# 23 code count per document median and 30 mean\n","\n","c1_trans_model = 'gbert'\n","c2_trans_model = 'gbert'\n","c3_trans_model = 'multi'\n","\n","folder_name = 'gbert_gbert_multi_code_rep'\n","feature_creation_type = 'triple_code_rep_fc'\n","\n","# Initalize Model Name and Tokenizer\n","trans_options = {'multi': ['Unbabel/xlm-roberta-comet-small', XLMRobertaTokenizer],\n","                 'cbert': ['microsoft/codebert-base', RobertaTokenizer],\n","                 'gbert': ['microsoft/graphcodebert-base', RobertaTokenizer],\n","                 't5': ['Salesforce/codet5-base', RobertaTokenizer]}\n","                 \n","fc_reqs = {'triple_code_rep_fc': [protobuf_handler.write_triple_set_to_tfrecords,\n","                                  protobuf_handler.decode_triple_protobuf,\n","                                  protobuf_handler.parse_triple_tensor_arrays],\n","           'dual_code_rep_fc': [protobuf_handler.write_dual_set_to_tfrecords,\n","                                protobuf_handler.decode_dual_protobuf,\n","                                protobuf_handler.parse_dual_tensor_arrays]}\n","\n","write_tensor_arrays_to_tfrecords = fc_reqs[feature_creation_type][0]\n","decode_protobuf = fc_reqs[feature_creation_type][1]\n","parse_tensor_arrays = fc_reqs[feature_creation_type][2]\n","\n","selected_trans = trans_options[c1_trans_model] \n","c1_trans_name = selected_trans[0]\n","c1_tokenizer_class = selected_trans[1]\n","c1_tokenizer = c1_tokenizer_class.from_pretrained(c1_trans_name)\n","\n","selected_trans = trans_options[c2_trans_model] \n","c2_trans_name = selected_trans[0]\n","c2_tokenizer_class = selected_trans[1]\n","c2_tokenizer = c2_tokenizer_class.from_pretrained(c2_trans_name)\n","\n","selected_trans = trans_options[c3_trans_model] \n","c3_trans_name = selected_trans[0]\n","c3_tokenizer_class = selected_trans[1]\n","c3_tokenizer = c3_tokenizer_class.from_pretrained(c3_trans_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gofzbex6i26O"},"outputs":[],"source":["# Add custom tokens\n","custom_tokens = ['<d>', '<c>', '[DIVIDER]', '[EMPTY]', '[EMOJI]']\n","\n","c1_tokenizer = pipeline.add_custom_tokens_to_tokenizer(c1_tokenizer, custom_tokens)\n","c2_tokenizer = pipeline.add_custom_tokens_to_tokenizer(c2_tokenizer, custom_tokens)\n","c3_tokenizer = pipeline.add_custom_tokens_to_tokenizer(c3_tokenizer, custom_tokens + ['\\n'])"]},{"cell_type":"markdown","metadata":{"id":"z6ICdV5L2TcO"},"source":["### Create One Dataset to Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RDGCY9r6wvxl"},"outputs":[],"source":["# Load data to memory and preprocess text (formatting and stemming)\n","\n","start_idx = 0\n","end_idx = 3000\n","\n","doc_ids, cell_metadata, unprocessed_text = pipeline.load_and_parse_data(data_paths, start_idx, end_idx)\n","code_markdown_locs = [cell for cells in cell_metadata for cell in cells.values()]\n","\n","text = pipeline.preprocess_text(unprocessed_text, code_markdown_locs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F8QE3kGTZ4W0"},"outputs":[],"source":["# Prepare Features and Labels\n","c1_input_ids = pipeline.encode_text_for_input_ids(text, c1_tokenizer, disable_print=True)\n","c2_input_ids = pipeline.encode_text_for_input_ids(text, c2_tokenizer, disable_print=True)\n","c3_input_ids = pipeline.encode_text_for_input_ids(text, c3_tokenizer)\n","\n","md_pct_ranks = dual_code_rep_fc.get_md_pct_ranks(doc_ids, cell_metadata, df_orders)\n","c1_groupings, c2_groupings, c3_groupings, labels = triple_code_rep_fc.collect_md_code_groupings(c1_input_ids,\n","                                                                                                c2_input_ids,\n","                                                                                                c3_input_ids,\n","                                                                                                cell_metadata,\n","                                                                                                md_pct_ranks)\n","c1_features = triple_code_rep_fc.create_features(c1_groupings, c1_tokenizer, md_len=70, disable_print=True)\n","c2_features = triple_code_rep_fc.create_features(c2_groupings, c2_tokenizer, md_len=70, disable_print=True)\n","c3_features = triple_code_rep_fc.create_features(c3_groupings, c3_tokenizer, md_len=130)\n","features = tuple((c1_features[0], c1_features[1],\n","                  c2_features[0], c2_features[1],\n","                  c3_features[0], c3_features[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sngGjWG981I5"},"outputs":[],"source":["sample_filepath = f'/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/tf_data/{folder_name}/{start_idx}-{end_idx}.tfrecords'\n","write_tensor_arrays_to_tfrecords(features, labels, sample_filepath)\n","print(f\"Finished dataset for {start_idx}-{end_idx}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aNSu7hSj1-X3"},"outputs":[],"source":["# # Prepare Features and Labels\n","# c1_input_ids = pipeline.encode_text_for_input_ids(text, c1_tokenizer, disable_print=True)\n","# c2_input_ids = pipeline.encode_text_for_input_ids(text, c2_tokenizer)\n","\n","# md_pct_ranks = dual_code_rep_fc.get_md_pct_ranks(doc_ids, cell_metadata, df_orders)\n","# c1_groupings, c2_groupings, labels = dual_code_rep_fc.collect_md_code_groupings(c1_input_ids,\n","#                                                                                 c2_input_ids,\n","#                                                                                 cell_metadata,\n","#                                                                                 md_pct_ranks)\n","# c1_features = dual_code_rep_fc.create_features(c1_groupings, c1_tokenizer, disable_print=True)\n","# c2_features = dual_code_rep_fc.create_features(c2_groupings, c2_tokenizer)\n","# features = tuple((c1_features[0], c1_features[1],\n","#                   c2_features[0], c2_features[1]))"]},{"cell_type":"markdown","metadata":{"id":"iGEo6E7TTKJD"},"source":["### Double Check Created Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5paKSpO-2hmJ"},"outputs":[],"source":["# Take a look at the unparsed daata\n","print(f'Markdown II:\\n{features[0]}\\n')\n","print(f'Markdown AM:\\n{features[1]}\\n')\n","print(f'Code II:\\n{features[2]}\\n')\n","print(f'Code AM:\\n{features[3]}\\n')\n","print(f'Labels:\\n{labels[:10]}\\n')\n","print(f'Labels Shapes (Please Ensure 2D): {labels.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RTrCD3r02ehC"},"outputs":[],"source":["# Manually parse TFRecords\n","auto = tf.data.experimental.AUTOTUNE\n","batch_size = 32\n","epochs = 2\n","print(f'Sample Filepath is {sample_filepath}')\n","\n","sample_dataset = (tf.data.TFRecordDataset(sample_filepath, num_parallel_reads=auto)\n","                  .map(decode_protobuf, num_parallel_calls=auto)\n","                  .map(parse_tensor_arrays, num_parallel_calls=auto)\n","                  .shuffle(len(labels)+1, seed=42)\n","                  .repeat(epochs)\n","                  .batch(batch_size, drop_remainder=True, num_parallel_calls=auto)\n","                  .prefetch(auto))\n","print(sample_dataset)\n","\n","for batch_features, batch_labels in sample_dataset.take(1):\n","  print(f'\\nParsed MD II:\\n{batch_features[0]}\\n')\n","  print(f'\\nFirst MD II:\\n{batch_features[0][0]}\\n')\n","\n","  print(f'Parsed MD AM:\\n{batch_features[1]}\\n')\n","  print(f'\\nFirst MD AM:\\n{batch_features[1][0]}\\n')\n","\n","  print(f'Parsed Code II:\\n{batch_features[2]}\\n')\n","  print(f'Parsed Code AM:\\n{batch_features[3]}\\n')\n","  \n","  print(f'Parsed Labels:\\n{batch_labels}\\n')\n","  print(f'\\nFirst Label:\\n{batch_labels[0]}\\n')\n","\n","# Remove sample file to avoid noise in final TFRecords folder\n","try:\n","  os.remove(sample_filepath)\n","  print('Deleted sample file to avoid noise creation in final TFRecords folder.')\n","except:\n","  print('Sample file has already been deleted.')"]},{"cell_type":"markdown","metadata":{"id":"y52ine_b2Wtn"},"source":["### Create all TFRecord Datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"s1W8W4NPt1la"},"outputs":[],"source":["# Create all 11 TFRecords Datasets\n","avg_samples_per_file = []\n","jumpstart = 0\n","\n","for i in range(len(all_dataset_splits[jumpstart:-1])):\n","  start_idx = all_dataset_splits[i+jumpstart]\n","  end_idx = all_dataset_splits[i+1+jumpstart]\n","  filepath = f'/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/tf_data/{folder_name}/{start_idx}-{end_idx}.tfrecords'\n","\n","  doc_ids, cell_metadata, text = pipeline.load_and_parse_data(data_paths, start_idx, end_idx)\n","  code_markdown_locs = [cell for cells in cell_metadata for cell in cells.values()]\n","  text = pipeline.preprocess_text(text, code_markdown_locs)\n","  \n","  # Prepare Features and Labels\n","  c1_input_ids = pipeline.encode_text_for_input_ids(text, c1_tokenizer, disable_print=True)\n","  c2_input_ids = pipeline.encode_text_for_input_ids(text, c2_tokenizer, disable_print=True)\n","  c3_input_ids = pipeline.encode_text_for_input_ids(text, c3_tokenizer)\n","\n","  md_pct_ranks = dual_code_rep_fc.get_md_pct_ranks(doc_ids, cell_metadata, df_orders)\n","  c1_groupings, c2_groupings, c3_groupings, labels = triple_code_rep_fc.collect_md_code_groupings(c1_input_ids,\n","                                                                                                  c2_input_ids,\n","                                                                                                  c3_input_ids,\n","                                                                                                  cell_metadata,\n","                                                                                                  md_pct_ranks)\n","  c1_features = triple_code_rep_fc.create_features(c1_groupings, c1_tokenizer, md_len=70, disable_print=True)\n","  c2_features = triple_code_rep_fc.create_features(c2_groupings, c2_tokenizer, md_len=70, disable_print=True)\n","  c3_features = triple_code_rep_fc.create_features(c3_groupings, c3_tokenizer, md_len=130)\n","  features = tuple((c1_features[0], c1_features[1],\n","                    c2_features[0], c2_features[1],\n","                    c3_features[0], c3_features[1]))\n","\n","  # Write Features to Protobufs\n","  avg_samples_per_file.append(len(features[0]))\n","  write_tensor_arrays_to_tfrecords(features, labels, filepath)\n","  print(f\"Finished dataset for {start_idx}-{end_idx}\\n\")\n","print('All requested datasets have been recoreded as TFRecords!')\n","print(f'The average samples per file was {sum(avg_samples_per_file)/len(avg_samples_per_file):.2f}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMrLyaVetPj0"},"outputs":[],"source":["# Save Test IDS for Post Processing\n","# test_ids_path = '/content/drive/MyDrive/Colab Notebooks/ML Projects/Google AI4Code/data/test_ids.yaml'\n","# test_id_idxs = all_dataset_splits[-17:]\n","\n","# test_ids = {}\n","# test_ids['ids'] = [test_id_idx.stem for test_id_idx in data_paths[test_id_idxs[0]:test_id_idxs[-1]]]\n","\n","# with open(test_ids_path, 'r+') as stream:\n","#   try:\n","#     yaml.dump(test_ids, stream, default_flow_style=False)\n","#   except yaml.YAMLError as error:\n","#     print(error)\n","# print('Succesfully dumped test ids to yaml file')"]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"Feature Creation.ipynb","provenance":[],"authorship_tag":"ABX9TyO0MbjiONpdTR+rCEYuPyUw"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}